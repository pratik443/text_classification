{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text_Classification_GRU.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vYpjbEH1jU_3"},"source":["import os \n","os.chdir(\"/\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fEi4zgVRjdCE","outputId":"f3c07d91-71c2-4652-f738-63a2a336f196"},"source":["!wget -O sample.csv https://gist.githubusercontent.com/hskalin/ed2d2af6d9a14fe4487fdfdadbf84ea1/raw/405ae905047b686a1aa24ce78c363d4ad6bbc61e/sample.csv"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-11-09 16:35:23--  https://gist.githubusercontent.com/hskalin/ed2d2af6d9a14fe4487fdfdadbf84ea1/raw/405ae905047b686a1aa24ce78c363d4ad6bbc61e/sample.csv\n","Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 37393 (37K) [text/plain]\n","Saving to: ‘sample.csv’\n","\n","sample.csv          100%[===================>]  36.52K  --.-KB/s    in 0.003s  \n","\n","2021-11-09 16:35:23 (12.2 MB/s) - ‘sample.csv’ saved [37393/37393]\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sigm2rb_jdpa","outputId":"5f33e5f2-f7d3-40c3-dbe2-95a29d9c5c07"},"source":["!pip install deepspeed"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deepspeed\n","  Downloading deepspeed-0.5.5.tar.gz (488 kB)\n","\u001b[K     |████████████████████████████████| 488 kB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.9.0+cu111)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed) (4.62.3)\n","Collecting tensorboardX==1.8\n","  Downloading tensorboardX-1.8-py2.py3-none-any.whl (216 kB)\n","\u001b[K     |████████████████████████████████| 216 kB 42.7 MB/s \n","\u001b[?25hCollecting ninja\n","  Downloading ninja-1.10.2.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n","\u001b[K     |████████████████████████████████| 108 kB 47.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.19.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from deepspeed) (5.4.8)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deepspeed) (21.0)\n","Collecting triton\n","  Downloading triton-1.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[K     |████████████████████████████████| 18.2 MB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed) (1.15.0)\n","Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed) (3.17.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deepspeed) (2.4.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->deepspeed) (3.7.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from triton->deepspeed) (3.3.0)\n","Building wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for deepspeed: filename=deepspeed-0.5.5-py3-none-any.whl size=496215 sha256=fbe74329b64a60fe643c863a27ab7160a32a614d37c4b3b7bf890bfdc7aa22d0\n","  Stored in directory: /root/.cache/pip/wheels/65/9a/da/f8470dc64c7deabe748f02291ed708b09887740f002fd770e3\n","Successfully built deepspeed\n","Installing collected packages: triton, tensorboardX, ninja, deepspeed\n","Successfully installed deepspeed-0.5.5 ninja-1.10.2.2 tensorboardX-1.8 triton-1.1.1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpiGqJErjeM8","outputId":"b3ccf0e2-df2b-4a90-d8e0-bc3c9e1d0e49"},"source":["!deepspeed textclassification_gru.py --deepspeed_config ds_config.json "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[2021-11-09 17:23:57,784] [WARNING] [runner.py:121:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n","[2021-11-09 17:23:57,804] [INFO] [runner.py:355:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 textclassification_gru.py --deepspeed_config ds_config.json\n","[2021-11-09 17:23:58,746] [INFO] [launch.py:73:main] 0 NCCL_VERSION 2.7.8\n","[2021-11-09 17:23:58,747] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n","[2021-11-09 17:23:58,747] [INFO] [launch.py:89:main] nnodes=1, num_local_procs=1, node_rank=0\n","[2021-11-09 17:23:58,747] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n","[2021-11-09 17:23:58,747] [INFO] [launch.py:102:main] dist_world_size=1\n","[2021-11-09 17:23:58,747] [INFO] [launch.py:105:main] Setting CUDA_VISIBLE_DEVICES=0\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[2021-11-09 17:24:03,236] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed info: version=0.5.5, git-hash=unknown, git-branch=unknown\n","[2021-11-09 17:24:03,236] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl\n","[2021-11-09 17:24:05,412] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed groups\n","[2021-11-09 17:24:05,412] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed model parallel group with size 1\n","[2021-11-09 17:24:05,414] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed expert parallel group with size 1\n","[2021-11-09 17:24:05,414] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n","[2021-11-09 17:24:05,415] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n","[2021-11-09 17:24:05,487] [INFO] [engine.py:208:__init__] DeepSpeed Flops Profiler Enabled: False\n","Using /root/.cache/torch_extensions as PyTorch extensions root...\n","Detected CUDA files, patching ldflags\n","Emitting ninja build file /root/.cache/torch_extensions/fused_adam/build.ninja...\n","Building extension module fused_adam...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","ninja: no work to do.\n","Loading extension module fused_adam...\n","Time to load fused_adam op: 0.27552008628845215 seconds\n","[2021-11-09 17:24:06,640] [INFO] [engine.py:886:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n","[2021-11-09 17:24:06,640] [INFO] [engine.py:893:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n","[2021-11-09 17:24:06,641] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n","[2021-11-09 17:24:06,641] [INFO] [engine.py:596:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n","[2021-11-09 17:24:06,641] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f4090596e50>\n","[2021-11-09 17:24:06,641] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.8, 0.999]]\n","[2021-11-09 17:24:06,641] [INFO] [config.py:958:print] DeepSpeedEngine configuration:\n","[2021-11-09 17:24:06,642] [INFO] [config.py:962:print]   activation_checkpointing_config  {\n","    \"partition_activations\": false, \n","    \"contiguous_memory_optimization\": false, \n","    \"cpu_checkpointing\": false, \n","    \"number_checkpoints\": null, \n","    \"synchronize_checkpoint_boundary\": false, \n","    \"profile\": false\n","}\n","[2021-11-09 17:24:06,642] [INFO] [config.py:962:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n","[2021-11-09 17:24:06,642] [INFO] [config.py:962:print]   allreduce_always_fp32 ........ False\n","[2021-11-09 17:24:06,642] [INFO] [config.py:962:print]   amp_enabled .................. False\n","[2021-11-09 17:24:06,642] [INFO] [config.py:962:print]   amp_params ................... False\n","[2021-11-09 17:24:06,642] [INFO] [config.py:962:print]   bfloat16_enabled ............. False\n","[2021-11-09 17:24:06,642] [INFO] [config.py:962:print]   checkpoint_tag_validation_enabled  True\n","[2021-11-09 17:24:06,642] [INFO] [config.py:962:print]   checkpoint_tag_validation_fail  False\n","[2021-11-09 17:24:06,642] [INFO] [config.py:962:print]   curriculum_enabled ........... False\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   curriculum_params ............ False\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   dataloader_drop_last ......... False\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   disable_allgather ............ False\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   dump_state ................... False\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   dynamic_loss_scale_args ...... None\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   eigenvalue_enabled ........... False\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   eigenvalue_gas_boundary_resolution  1\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   eigenvalue_layer_name ........ bert.encoder.layer\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   eigenvalue_layer_num ......... 0\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   eigenvalue_max_iter .......... 100\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   eigenvalue_stability ......... 1e-06\n","[2021-11-09 17:24:06,643] [INFO] [config.py:962:print]   eigenvalue_tol ............... 0.01\n","[2021-11-09 17:24:06,644] [INFO] [config.py:962:print]   eigenvalue_verbose ........... False\n","[2021-11-09 17:24:06,644] [INFO] [config.py:962:print]   elasticity_enabled ........... False\n","[2021-11-09 17:24:06,644] [INFO] [config.py:962:print]   flops_profiler_config ........ {\n","    \"enabled\": false, \n","    \"profile_step\": 1, \n","    \"module_depth\": -1, \n","    \"top_modules\": 1, \n","    \"detailed\": true, \n","    \"output_file\": null\n","}\n","[2021-11-09 17:24:06,644] [INFO] [config.py:962:print]   fp16_enabled ................. False\n","[2021-11-09 17:24:06,644] [INFO] [config.py:962:print]   fp16_master_weights_and_gradients  False\n","[2021-11-09 17:24:06,644] [INFO] [config.py:962:print]   fp16_mixed_quantize .......... False\n","[2021-11-09 17:24:06,644] [INFO] [config.py:962:print]   global_rank .................. 0\n","[2021-11-09 17:24:06,644] [INFO] [config.py:962:print]   gradient_accumulation_steps .. 1\n","[2021-11-09 17:24:06,644] [INFO] [config.py:962:print]   gradient_clipping ............ 0.0\n","[2021-11-09 17:24:06,644] [INFO] [config.py:962:print]   gradient_predivide_factor .... 1.0\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   initial_dynamic_scale ........ 4294967296\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   loss_scale ................... 0\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   memory_breakdown ............. False\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   optimizer_legacy_fusion ...... False\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   optimizer_name ............... adam\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   pld_enabled .................. False\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   pld_params ................... False\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   prescale_gradients ........... False\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   quantize_change_rate ......... 0.001\n","[2021-11-09 17:24:06,645] [INFO] [config.py:962:print]   quantize_groups .............. 1\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   quantize_offset .............. 1000\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   quantize_period .............. 1000\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   quantize_rounding ............ 0\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   quantize_start_bits .......... 16\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   quantize_target_bits ......... 8\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   quantize_training_enabled .... False\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   quantize_type ................ 0\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   quantize_verbose ............. False\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   scheduler_name ............... WarmupLR\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   sparse_attention ............. None\n","[2021-11-09 17:24:06,646] [INFO] [config.py:962:print]   sparse_gradients_enabled ..... False\n","[2021-11-09 17:24:06,647] [INFO] [config.py:962:print]   steps_per_print .............. 2000\n","[2021-11-09 17:24:06,647] [INFO] [config.py:962:print]   tensorboard_enabled .......... False\n","[2021-11-09 17:24:06,647] [INFO] [config.py:962:print]   tensorboard_job_name ......... DeepSpeedJobName\n","[2021-11-09 17:24:06,647] [INFO] [config.py:962:print]   tensorboard_output_path ...... \n","[2021-11-09 17:24:06,647] [INFO] [config.py:962:print]   train_batch_size ............. 1\n","[2021-11-09 17:24:06,647] [INFO] [config.py:962:print]   train_micro_batch_size_per_gpu  1\n","[2021-11-09 17:24:06,647] [INFO] [config.py:962:print]   use_quantizer_kernel ......... False\n","[2021-11-09 17:24:06,647] [INFO] [config.py:962:print]   wall_clock_breakdown ......... False\n","[2021-11-09 17:24:06,647] [INFO] [config.py:962:print]   world_size ................... 1\n","[2021-11-09 17:24:06,647] [INFO] [config.py:962:print]   zero_allow_untested_optimizer  False\n","[2021-11-09 17:24:06,648] [INFO] [config.py:962:print]   zero_config .................. {\n","    \"stage\": 0, \n","    \"contiguous_gradients\": true, \n","    \"reduce_scatter\": true, \n","    \"reduce_bucket_size\": 5.000000e+08, \n","    \"allgather_partitions\": true, \n","    \"allgather_bucket_size\": 5.000000e+08, \n","    \"overlap_comm\": false, \n","    \"load_from_fp32_weights\": true, \n","    \"elastic_checkpoint\": true, \n","    \"offload_param\": null, \n","    \"offload_optimizer\": null, \n","    \"sub_group_size\": 1.000000e+09, \n","    \"prefetch_bucket_size\": 5.000000e+07, \n","    \"param_persistence_threshold\": 1.000000e+05, \n","    \"max_live_parameters\": 1.000000e+09, \n","    \"max_reuse_distance\": 1.000000e+09, \n","    \"gather_fp16_weights_on_model_save\": false, \n","    \"ignore_unused_parameters\": true, \n","    \"round_robin_gradients\": false, \n","    \"legacy_stage1\": false\n","}\n","[2021-11-09 17:24:06,648] [INFO] [config.py:962:print]   zero_enabled ................. False\n","[2021-11-09 17:24:06,648] [INFO] [config.py:962:print]   zero_optimization_stage ...... 0\n","[2021-11-09 17:24:06,648] [INFO] [config.py:969:print]   json = {\n","    \"train_batch_size\": 1, \n","    \"steps_per_print\": 2.000000e+03, \n","    \"optimizer\": {\n","        \"type\": \"Adam\", \n","        \"params\": {\n","            \"lr\": 0.001, \n","            \"betas\": [0.8, 0.999], \n","            \"eps\": 1e-08, \n","            \"weight_decay\": 3e-07\n","        }\n","    }, \n","    \"scheduler\": {\n","        \"type\": \"WarmupLR\", \n","        \"params\": {\n","            \"warmup_min_lr\": 0, \n","            \"warmup_max_lr\": 0.001, \n","            \"warmup_num_steps\": 1000\n","        }\n","    }, \n","    \"wall_clock_breakdown\": false\n","}\n","Using /root/.cache/torch_extensions as PyTorch extensions root...\n","Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n","Building extension module utils...\n","Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n","ninja: no work to do.\n","Loading extension module utils...\n","Time to load utils op: 0.2706179618835449 seconds\n","Epoch1\n","Epoch ran :2\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 0.2650,  0.2143, -0.0458, -0.6783, -0.2874,  0.0592]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","0\n","Epoch2\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Epoch ran :3\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 0.2757,  0.2501, -0.0515, -0.7295, -0.3338, -0.0041]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","0\n","Epoch3\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Epoch ran :4\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 0.2994,  0.2566, -0.0310, -0.7640, -0.3726, -0.0530]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","0\n","Epoch4\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Epoch ran :5\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 0.3035,  0.2514,  0.0096, -0.8413, -0.4523, -0.0944]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","0\n","Epoch5\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Epoch ran :6\n","YO\n","[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n","[2021-11-09 17:24:08,592] [INFO] [logging.py:68:log_dist] [Rank 0] Saving model checkpoint: ./gru_big_model_3_with_padding_epoch_5.pth/global_step275/mp_rank_00_model_states.pt\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 0.4815,  0.6590, -0.4247, -0.4795, -0.0228, -0.4606]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","1\n","Epoch6\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Epoch ran :7\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 0.5079,  0.6189, -0.3756, -0.4570, -0.0169, -0.4547]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","1\n","Epoch7\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Epoch ran :8\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 3.1388e-01,  3.9628e-01, -2.3884e-04, -1.1615e+00, -7.4250e-01,\n","         -3.8070e-01]], device='cuda:0', grad_fn=<AddmmBackward>)\n","1\n","Epoch8\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Epoch ran :9\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 0.5249,  0.6375, -0.3518, -0.5341, -0.0942, -0.5350]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","1\n","Epoch9\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Epoch ran :10\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 0.4851,  0.8338, -0.4395, -0.8642, -0.3708, -0.8042]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","1\n","Epoch10\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Epoch ran :11\n","YO\n","[2021-11-09 17:24:10,146] [INFO] [logging.py:68:log_dist] [Rank 0] Saving model checkpoint: ./gru_big_model_3_with_padding_epoch_10.pth/global_step550/mp_rank_00_model_states.pt\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 0.4865,  0.8868, -0.4432, -0.9905, -0.4866, -0.9176]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","1\n","Epoch11\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Epoch ran :12\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 0.3713,  0.3237,  0.1644, -1.2883, -0.9063, -0.4855]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","0\n","Epoch12\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n","Epoch ran :13\n","Input vector\n","[[ 7 17 10 ... 27 27 27]]\n","Probs\n","tensor([[ 0.5564,  0.6608, -0.3009, -0.6836, -0.2458, -0.6807]],\n","       device='cuda:0', grad_fn=<AddmmBackward>)\n","1\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","              precision    recall  f1-score   support\n","\n","           1       0.00      0.00      0.00       0.0\n","           2       0.00      0.00      0.00       1.0\n","           3       0.00      0.00      0.00       4.0\n","           4       0.00      0.00      0.00       5.0\n","           5       0.00      0.00      0.00       4.0\n","\n","    accuracy                           0.00      14.0\n","   macro avg       0.00      0.00      0.00      14.0\n","weighted avg       0.00      0.00      0.00      14.0\n","\n","Index(['iter', ' loss'], dtype='object')\n"]}]}]}