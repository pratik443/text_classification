{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_CNN_Final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DptK-GYLokpW"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEvwnx76LNDn",
        "outputId": "e5e58ab6-c670-483f-e2f8-8afd0a71e2b2"
      },
      "source": [
        "!wget -O sample.csv https://gist.githubusercontent.com/hskalin/ed2d2af6d9a14fe4487fdfdadbf84ea1/raw/405ae905047b686a1aa24ce78c363d4ad6bbc61e/sample.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-05 19:49:39--  https://gist.githubusercontent.com/hskalin/ed2d2af6d9a14fe4487fdfdadbf84ea1/raw/405ae905047b686a1aa24ce78c363d4ad6bbc61e/sample.csv\n",
            "Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 37393 (37K) [text/plain]\n",
            "Saving to: ‘sample.csv’\n",
            "\n",
            "sample.csv          100%[===================>]  36.52K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-11-05 19:49:39 (11.6 MB/s) - ‘sample.csv’ saved [37393/37393]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6OtROfjJnn7",
        "outputId": "c83bdae3-e65e-4f9d-9385-1cbca66983a4"
      },
      "source": [
        "!pip install deepspeed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepspeed\n",
            "  Downloading deepspeed-0.5.5.tar.gz (488 kB)\n",
            "\u001b[K     |████████████████████████████████| 488 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.9.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed) (4.62.3)\n",
            "Collecting tensorboardX==1.8\n",
            "  Downloading tensorboardX-1.8-py2.py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 44.2 MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.10.2.2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.19.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from deepspeed) (5.4.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deepspeed) (21.0)\n",
            "Collecting triton\n",
            "  Downloading triton-1.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.2 MB 47 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed) (3.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deepspeed) (2.4.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->deepspeed) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from triton->deepspeed) (3.3.0)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.5.5-py3-none-any.whl size=496214 sha256=e7882307e74d9a0ac70042553ab340ce9a55caf7564ecb7172ee5c0eeef7e5cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/9a/da/f8470dc64c7deabe748f02291ed708b09887740f002fd770e3\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: triton, tensorboardX, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.5.5 ninja-1.10.2.2 tensorboardX-1.8 triton-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxEb1acoJj1Z",
        "outputId": "fbe60e0f-54a8-4ef7-f2b9-3515dca8f9f0"
      },
      "source": [
        "!deepspeed textclassification_cnn.py --deepspeed_config ds_config.json "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2021-11-05 19:49:47,313] [WARNING] [runner.py:121:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2021-11-05 19:49:47,336] [INFO] [runner.py:355:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 textclassification_cnn.py --deepspeed_config ds_config.json\n",
            "[2021-11-05 19:49:48,308] [INFO] [launch.py:73:main] 0 NCCL_VERSION 2.7.8\n",
            "[2021-11-05 19:49:48,308] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2021-11-05 19:49:48,308] [INFO] [launch.py:89:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2021-11-05 19:49:48,308] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2021-11-05 19:49:48,308] [INFO] [launch.py:102:main] dist_world_size=1\n",
            "[2021-11-05 19:49:48,308] [INFO] [launch.py:105:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[2021-11-05 19:49:52,834] [INFO] [logging.py:68:log_dist] [Rank -1] DeepSpeed info: version=0.5.5, git-hash=unknown, git-branch=unknown\n",
            "[2021-11-05 19:49:52,835] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl\n",
            "[2021-11-05 19:50:07,302] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed groups\n",
            "[2021-11-05 19:50:07,303] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed model parallel group with size 1\n",
            "[2021-11-05 19:50:07,304] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed expert parallel group with size 1\n",
            "[2021-11-05 19:50:07,304] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
            "[2021-11-05 19:50:07,305] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n",
            "[2021-11-05 19:50:07,393] [INFO] [engine.py:208:__init__] DeepSpeed Flops Profiler Enabled: False\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/fused_adam...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/fused_adam/build.ninja...\n",
            "Building extension module fused_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_37,code=compute_37 -gencode=arch=compute_37,code=sm_37 --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode=arch=compute_37,code=sm_37 -gencode=arch=compute_37,code=compute_37 -std=c++14 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o \n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o \n",
            "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
            "Loading extension module fused_adam...\n",
            "Time to load fused_adam op: 37.250205516815186 seconds\n",
            "[2021-11-05 19:50:45,548] [INFO] [engine.py:886:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
            "[2021-11-05 19:50:45,549] [INFO] [engine.py:893:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
            "[2021-11-05 19:50:45,549] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
            "[2021-11-05 19:50:45,549] [INFO] [engine.py:596:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2021-11-05 19:50:45,549] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fe0b675b990>\n",
            "[2021-11-05 19:50:45,549] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.8, 0.999]]\n",
            "[2021-11-05 19:50:45,549] [INFO] [config.py:958:print] DeepSpeedEngine configuration:\n",
            "[2021-11-05 19:50:45,550] [INFO] [config.py:962:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2021-11-05 19:50:45,550] [INFO] [config.py:962:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2021-11-05 19:50:45,550] [INFO] [config.py:962:print]   allreduce_always_fp32 ........ False\n",
            "[2021-11-05 19:50:45,550] [INFO] [config.py:962:print]   amp_enabled .................. False\n",
            "[2021-11-05 19:50:45,550] [INFO] [config.py:962:print]   amp_params ................... False\n",
            "[2021-11-05 19:50:45,550] [INFO] [config.py:962:print]   bfloat16_enabled ............. False\n",
            "[2021-11-05 19:50:45,551] [INFO] [config.py:962:print]   checkpoint_tag_validation_enabled  True\n",
            "[2021-11-05 19:50:45,551] [INFO] [config.py:962:print]   checkpoint_tag_validation_fail  False\n",
            "[2021-11-05 19:50:45,551] [INFO] [config.py:962:print]   curriculum_enabled ........... False\n",
            "[2021-11-05 19:50:45,551] [INFO] [config.py:962:print]   curriculum_params ............ False\n",
            "[2021-11-05 19:50:45,551] [INFO] [config.py:962:print]   dataloader_drop_last ......... False\n",
            "[2021-11-05 19:50:45,551] [INFO] [config.py:962:print]   disable_allgather ............ False\n",
            "[2021-11-05 19:50:45,551] [INFO] [config.py:962:print]   dump_state ................... False\n",
            "[2021-11-05 19:50:45,551] [INFO] [config.py:962:print]   dynamic_loss_scale_args ...... None\n",
            "[2021-11-05 19:50:45,551] [INFO] [config.py:962:print]   eigenvalue_enabled ........... False\n",
            "[2021-11-05 19:50:45,551] [INFO] [config.py:962:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2021-11-05 19:50:45,551] [INFO] [config.py:962:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2021-11-05 19:50:45,552] [INFO] [config.py:962:print]   eigenvalue_layer_num ......... 0\n",
            "[2021-11-05 19:50:45,552] [INFO] [config.py:962:print]   eigenvalue_max_iter .......... 100\n",
            "[2021-11-05 19:50:45,552] [INFO] [config.py:962:print]   eigenvalue_stability ......... 1e-06\n",
            "[2021-11-05 19:50:45,552] [INFO] [config.py:962:print]   eigenvalue_tol ............... 0.01\n",
            "[2021-11-05 19:50:45,552] [INFO] [config.py:962:print]   eigenvalue_verbose ........... False\n",
            "[2021-11-05 19:50:45,552] [INFO] [config.py:962:print]   elasticity_enabled ........... False\n",
            "[2021-11-05 19:50:45,552] [INFO] [config.py:962:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2021-11-05 19:50:45,552] [INFO] [config.py:962:print]   fp16_enabled ................. False\n",
            "[2021-11-05 19:50:45,552] [INFO] [config.py:962:print]   fp16_master_weights_and_gradients  False\n",
            "[2021-11-05 19:50:45,552] [INFO] [config.py:962:print]   fp16_mixed_quantize .......... False\n",
            "[2021-11-05 19:50:45,553] [INFO] [config.py:962:print]   global_rank .................. 0\n",
            "[2021-11-05 19:50:45,553] [INFO] [config.py:962:print]   gradient_accumulation_steps .. 1\n",
            "[2021-11-05 19:50:45,553] [INFO] [config.py:962:print]   gradient_clipping ............ 0.0\n",
            "[2021-11-05 19:50:45,553] [INFO] [config.py:962:print]   gradient_predivide_factor .... 1.0\n",
            "[2021-11-05 19:50:45,553] [INFO] [config.py:962:print]   initial_dynamic_scale ........ 4294967296\n",
            "[2021-11-05 19:50:45,553] [INFO] [config.py:962:print]   loss_scale ................... 0\n",
            "[2021-11-05 19:50:45,553] [INFO] [config.py:962:print]   memory_breakdown ............. False\n",
            "[2021-11-05 19:50:45,553] [INFO] [config.py:962:print]   optimizer_legacy_fusion ...... False\n",
            "[2021-11-05 19:50:45,554] [INFO] [config.py:962:print]   optimizer_name ............... adam\n",
            "[2021-11-05 19:50:45,554] [INFO] [config.py:962:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n",
            "[2021-11-05 19:50:45,554] [INFO] [config.py:962:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2021-11-05 19:50:45,554] [INFO] [config.py:962:print]   pld_enabled .................. False\n",
            "[2021-11-05 19:50:45,554] [INFO] [config.py:962:print]   pld_params ................... False\n",
            "[2021-11-05 19:50:45,554] [INFO] [config.py:962:print]   prescale_gradients ........... False\n",
            "[2021-11-05 19:50:45,554] [INFO] [config.py:962:print]   quantize_change_rate ......... 0.001\n",
            "[2021-11-05 19:50:45,554] [INFO] [config.py:962:print]   quantize_groups .............. 1\n",
            "[2021-11-05 19:50:45,554] [INFO] [config.py:962:print]   quantize_offset .............. 1000\n",
            "[2021-11-05 19:50:45,554] [INFO] [config.py:962:print]   quantize_period .............. 1000\n",
            "[2021-11-05 19:50:45,554] [INFO] [config.py:962:print]   quantize_rounding ............ 0\n",
            "[2021-11-05 19:50:45,555] [INFO] [config.py:962:print]   quantize_start_bits .......... 16\n",
            "[2021-11-05 19:50:45,555] [INFO] [config.py:962:print]   quantize_target_bits ......... 8\n",
            "[2021-11-05 19:50:45,555] [INFO] [config.py:962:print]   quantize_training_enabled .... False\n",
            "[2021-11-05 19:50:45,555] [INFO] [config.py:962:print]   quantize_type ................ 0\n",
            "[2021-11-05 19:50:45,555] [INFO] [config.py:962:print]   quantize_verbose ............. False\n",
            "[2021-11-05 19:50:45,555] [INFO] [config.py:962:print]   scheduler_name ............... WarmupLR\n",
            "[2021-11-05 19:50:45,555] [INFO] [config.py:962:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}\n",
            "[2021-11-05 19:50:45,555] [INFO] [config.py:962:print]   sparse_attention ............. None\n",
            "[2021-11-05 19:50:45,555] [INFO] [config.py:962:print]   sparse_gradients_enabled ..... False\n",
            "[2021-11-05 19:50:45,555] [INFO] [config.py:962:print]   steps_per_print .............. 2000\n",
            "[2021-11-05 19:50:45,556] [INFO] [config.py:962:print]   tensorboard_enabled .......... False\n",
            "[2021-11-05 19:50:45,556] [INFO] [config.py:962:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2021-11-05 19:50:45,556] [INFO] [config.py:962:print]   tensorboard_output_path ...... \n",
            "[2021-11-05 19:50:45,556] [INFO] [config.py:962:print]   train_batch_size ............. 1\n",
            "[2021-11-05 19:50:45,556] [INFO] [config.py:962:print]   train_micro_batch_size_per_gpu  1\n",
            "[2021-11-05 19:50:45,556] [INFO] [config.py:962:print]   use_quantizer_kernel ......... False\n",
            "[2021-11-05 19:50:45,556] [INFO] [config.py:962:print]   wall_clock_breakdown ......... False\n",
            "[2021-11-05 19:50:45,556] [INFO] [config.py:962:print]   world_size ................... 1\n",
            "[2021-11-05 19:50:45,556] [INFO] [config.py:962:print]   zero_allow_untested_optimizer  False\n",
            "[2021-11-05 19:50:45,557] [INFO] [config.py:962:print]   zero_config .................. {\n",
            "    \"stage\": 0, \n",
            "    \"contiguous_gradients\": true, \n",
            "    \"reduce_scatter\": true, \n",
            "    \"reduce_bucket_size\": 5.000000e+08, \n",
            "    \"allgather_partitions\": true, \n",
            "    \"allgather_bucket_size\": 5.000000e+08, \n",
            "    \"overlap_comm\": false, \n",
            "    \"load_from_fp32_weights\": true, \n",
            "    \"elastic_checkpoint\": true, \n",
            "    \"offload_param\": null, \n",
            "    \"offload_optimizer\": null, \n",
            "    \"sub_group_size\": 1.000000e+09, \n",
            "    \"prefetch_bucket_size\": 5.000000e+07, \n",
            "    \"param_persistence_threshold\": 1.000000e+05, \n",
            "    \"max_live_parameters\": 1.000000e+09, \n",
            "    \"max_reuse_distance\": 1.000000e+09, \n",
            "    \"gather_fp16_weights_on_model_save\": false, \n",
            "    \"ignore_unused_parameters\": true, \n",
            "    \"round_robin_gradients\": false, \n",
            "    \"legacy_stage1\": false\n",
            "}\n",
            "[2021-11-05 19:50:45,557] [INFO] [config.py:962:print]   zero_enabled ................. False\n",
            "[2021-11-05 19:50:45,557] [INFO] [config.py:962:print]   zero_optimization_stage ...... 0\n",
            "[2021-11-05 19:50:45,558] [INFO] [config.py:969:print]   json = {\n",
            "    \"train_batch_size\": 1, \n",
            "    \"steps_per_print\": 2.000000e+03, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"Adam\", \n",
            "        \"params\": {\n",
            "            \"lr\": 0.001, \n",
            "            \"betas\": [0.8, 0.999], \n",
            "            \"eps\": 1e-08, \n",
            "            \"weight_decay\": 3e-07\n",
            "        }\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 0, \n",
            "            \"warmup_max_lr\": 0.001, \n",
            "            \"warmup_num_steps\": 1000\n",
            "        }\n",
            "    }, \n",
            "    \"wall_clock_breakdown\": false\n",
            "}\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/utils...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
            "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 16.16605544090271 seconds\n",
            "Epoch1\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "Epoch ran :2\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.1736, 0.2527, 0.2193, 0.1210, 0.1197, 0.1138]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "Epoch2\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch ran :3\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.2082, 0.3643, 0.2391, 0.0636, 0.0645, 0.0603]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "Epoch3\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch ran :4\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.2264, 0.4520, 0.2281, 0.0312, 0.0324, 0.0299]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "Epoch4\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch ran :5\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.2371, 0.5012, 0.2088, 0.0176, 0.0184, 0.0169]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "Epoch5\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch ran :6\n",
            "YO\n",
            "[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n",
            "[2021-11-05 19:51:04,435] [INFO] [logging.py:68:log_dist] [Rank 0] Saving model checkpoint: ./cnn_big_model_100_with_padding_epoch_5.pth/global_step275/mp_rank_00_model_states.pt\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.2467, 0.5278, 0.1917, 0.0112, 0.0118, 0.0108]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "Epoch6\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch ran :7\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.2566, 0.5423, 0.1776, 0.0078, 0.0082, 0.0075]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "Epoch7\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch ran :8\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.2669, 0.5498, 0.1659, 0.0057, 0.0060, 0.0056]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "Epoch8\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch ran :9\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.2774, 0.5533, 0.1559, 0.0044, 0.0046, 0.0043]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "Epoch9\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch ran :10\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.2857, 0.5552, 0.1481, 0.0037, 0.0037, 0.0035]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "Epoch10\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch ran :11\n",
            "YO\n",
            "[2021-11-05 19:51:06,639] [INFO] [logging.py:68:log_dist] [Rank 0] Saving model checkpoint: ./cnn_big_model_100_with_padding_epoch_10.pth/global_step550/mp_rank_00_model_states.pt\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.2974, 0.5545, 0.1396, 0.0028, 0.0029, 0.0027]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "Epoch11\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch ran :12\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.3068, 0.5533, 0.1328, 0.0023, 0.0024, 0.0023]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "Epoch12\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "Epoch ran :13\n",
            "Input vector\n",
            "[[ 7 17 10 ... 27 27 27]]\n",
            "Probs\n",
            "tensor([[0.3156, 0.5519, 0.1267, 0.0020, 0.0020, 0.0019]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "1\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00       0.0\n",
            "           2       0.00      0.00      0.00       1.0\n",
            "           3       0.00      0.00      0.00       4.0\n",
            "           4       0.00      0.00      0.00       5.0\n",
            "           5       0.00      0.00      0.00       4.0\n",
            "\n",
            "    accuracy                           0.00      14.0\n",
            "   macro avg       0.00      0.00      0.00      14.0\n",
            "weighted avg       0.00      0.00      0.00      14.0\n",
            "\n",
            "Index(['iter', ' loss'], dtype='object')\n"
          ]
        }
      ]
    }
  ]
}